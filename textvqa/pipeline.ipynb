{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/tra220029p/tejaswin/miniconda3/envs/mobile/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import convert_ans_to_token, convert_ques_to_token, rotate, convert_token_to_ques, convert_token_to_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_json_file, get_specific_file, resize_align_bbox, get_tokens_with_boxes, create_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_BOX = [0, 0, 0, 0]\n",
    "max_seq_len = 512\n",
    "batch_size = 2\n",
    "target_size = (500,384) ## Note that, ViT would make it 224x224 so :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../textvqa_eval/\"\n",
    "\n",
    "train_ocr_json_path = os.path.join(base_path, 'TextVQA_Rosetta_OCR_v0.2_train.json')\n",
    "train_json_path = os.path.join(base_path, 'TextVQA_0.5.1_train.json')\n",
    "\n",
    "val_ocr_json_path = os.path.join(base_path, 'TextVQA_Rosetta_OCR_v0.2_val.json')\n",
    "val_json_path = os.path.join(base_path, 'TextVQA_0.5.1_val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ocr_json = json.load(open(train_ocr_json_path))['data']\n",
    "train_json = json.load(open(train_json_path))['data']\n",
    "\n",
    "val_ocr_json = json.load(open(val_ocr_json_path))['data']\n",
    "val_json = json.load(open(val_json_path))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_df = pd.DataFrame(train_json)\n",
    "train_ocr_json_df = pd.DataFrame(train_ocr_json)\n",
    "\n",
    "val_json_df = pd.DataFrame(val_json)\n",
    "val_ocr_json_df = pd.DataFrame(val_ocr_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_df['answers'] = train_json_df['answers'].apply(lambda x: \" \".join(list(map(str, x))))\n",
    "val_json_df['answers']   = val_json_df['answers'].apply(lambda x: \" \".join(list(map(str, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34602/34602 [00:06<00:00, 5289.37it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5466.71it/s]\n"
     ]
    }
   ],
   "source": [
    "## Dropping of the images which doesn't exist, might take some time\n",
    "\n",
    "base_img_path = os.path.join(base_path, 'train_images')\n",
    "\n",
    "train_json_df['path_exists'] = train_json_df['image_id'].progress_apply(lambda x: os.path.exists(os.path.join(base_img_path, x)+'.jpg'))\n",
    "train_json_df = train_json_df[train_json_df['path_exists']==True]\n",
    "\n",
    "val_json_df['path_exists'] = val_json_df['image_id'].progress_apply(lambda x: os.path.exists(os.path.join(base_img_path, x)+'.jpg'))\n",
    "val_json_df = val_json_df[val_json_df['path_exists']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the unused columns\n",
    "\n",
    "train_json_df.drop(columns = ['flickr_original_url', 'flickr_300k_url','image_classes', 'question_tokens', 'path_exists'\n",
    "                              ], axis = 1, inplace = True)\n",
    "val_json_df.drop(columns = ['flickr_original_url', 'flickr_300k_url','image_classes', 'question_tokens', 'path_exists'\n",
    "                              ], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting the json\n",
    "\n",
    "del train_json\n",
    "del train_ocr_json\n",
    "del val_json\n",
    "del val_ocr_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping for the purpose of feature extraction\n",
    "grouped_df = train_json_df.groupby('image_id')\n",
    "\n",
    "## Getting all the unique keys of the group by object\n",
    "keys = list(grouped_df.groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset class for TextVQA\n",
    "\n",
    "class TextVqaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_img_path, json_df, ocr_json_df, tokenizer, \n",
    "    transform = None, max_seq_length = 100, target_size = (500,384), fine_tune = True):\n",
    "        self.base_img_path = base_img_path\n",
    "        self.json_df = json_df\n",
    "        self.ocr_json_df = ocr_json_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.fine_tune = fine_tune\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_img = self.json_df.iloc[idx]['image_id']\n",
    "        ocr_token = self.ocr_json_df[self.ocr_json_df['image_id']==curr_img]['ocr_info'].values.tolist()[0]\n",
    "\n",
    "        boxes = []\n",
    "        words = []\n",
    "\n",
    "        current_group = self.json_df.iloc[idx]\n",
    "        width, height = current_group['image_width'], current_group['image_height']\n",
    "\n",
    "        for entry in ocr_token:\n",
    "            xmin, ymin, w, h, angle = entry['bounding_box']['top_left_x'], entry['bounding_box']['top_left_y'],  entry['bounding_box']['width'],  entry['bounding_box']['height'], entry['bounding_box']['rotation']\n",
    "            xmin, ymin,w, h = resize_align_bbox([xmin, ymin, w, h], 1, 1, width, height)\n",
    "            \n",
    "            x_centre = xmin + (w/2)\n",
    "            y_centre = ymin + (h/2)\n",
    "\n",
    "            ## print(\"The angle is:\", angle)\n",
    "            xmin, ymin = rotate([x_centre, y_centre], [xmin, ymin], angle)\n",
    "\n",
    "            xmax = xmin + w\n",
    "            ymax = ymin + h\n",
    "\n",
    "            ## Bounding boxes are normalized\n",
    "            curr_bbox = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(curr_bbox)\n",
    "            words.append(entry['word'])\n",
    "\n",
    "        img_path = os.path.join(self.base_img_path, curr_img)+'.jpg'\n",
    "        assert os.path.exists(img_path)==True, f'Make sure that the image exists at {img_path}!!'\n",
    "\n",
    "        if self.fine_tune:\n",
    "            ## For fine-tune stage, they use [0, 0, 1000, 1000] for all the bounding box\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = img.resize(self.target_size)\n",
    "            boxes = torch.zeros(self.max_seq_length, 4)\n",
    "            boxes[:, 2] = 1000\n",
    "            boxes[:, 3] = 1000\n",
    "            \n",
    "            words = \" \".join(words)\n",
    "            tokenized_words = self.tokenizer.encode(words, max_length = self.max_seq_length, \n",
    "                truncation = True, padding = 'max_length', return_tensors = 'pt')[0]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Flow for `self.fine_tune != False` is not defined!\")\n",
    "\n",
    "        ## Converting the boxes as per the format required for model input\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.int32)\n",
    "        width = (boxes[:, 2] - boxes[:, 0]).view(-1, 1)\n",
    "        height = (boxes[:, 3] - boxes[:, 1]).view(-1, 1)\n",
    "        boxes = torch.cat([boxes, width, height], axis = -1)\n",
    "\n",
    "        ## Clamping the value,as some of the box values are out of bound\n",
    "        boxes[:, 0] = torch.clamp(boxes[:, 0], min = 0, max = 1000)\n",
    "        boxes[:, 2] = torch.clamp(boxes[:, 2], min = 0, max = 1000)\n",
    "        boxes[:, 4] = torch.clamp(boxes[:, 4], min = 0, max = 1000)\n",
    "        \n",
    "        boxes[:, 1] = torch.clamp(boxes[:, 1], min = 0, max = 1000)\n",
    "        boxes[:, 3] = torch.clamp(boxes[:, 3], min = 0, max = 1000)\n",
    "        boxes[:, 5] = torch.clamp(boxes[:, 5], min = 0, max = 1000)\n",
    "        \n",
    "        ## Tensor tokenized words\n",
    "        tokenized_words = torch.as_tensor(tokenized_words, dtype=torch.int32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "\n",
    "        ## Getting the Question\n",
    "        question = current_group['question']   \n",
    "        question = convert_ques_to_token(question = question, tokenizer = self.tokenizer)\n",
    "\n",
    "        ## Getting the Answer\n",
    "        answer = current_group['answers']\n",
    "        answer = convert_ques_to_token(question = answer, tokenizer = self.tokenizer).long()\n",
    "\n",
    "        return {'img':img, 'boxes': boxes, 'tokenized_words': tokenized_words, \n",
    "                'question': question, 'answer': answer, 'id': torch.as_tensor(idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_tokenizer(from_pretrained):\n",
    "    if torch.distributed.is_initialized():\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            BertTokenizer.from_pretrained(\n",
    "                from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "            )\n",
    "        torch.distributed.barrier()\n",
    "    return BertTokenizer.from_pretrained(\n",
    "        from_pretrained, do_lower_case=\"uncased\" in from_pretrained\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_pretrained_tokenizer(\"bert-base-uncased\")\n",
    "\n",
    "train_ds = TextVqaDataset(base_img_path = base_img_path,\n",
    "                         json_df = train_json_df,\n",
    "                         ocr_json_df = train_ocr_json_df,\n",
    "                         tokenizer = tokenizer,\n",
    "                         transform = None, \n",
    "                         max_seq_length = max_seq_len, \n",
    "                         target_size = target_size\n",
    "                         )\n",
    "\n",
    "\n",
    "val_ds = TextVqaDataset(base_img_path = base_img_path,\n",
    "                        json_df = val_json_df,\n",
    "                        ocr_json_df = val_ocr_json_df,\n",
    "                        tokenizer = tokenizer,\n",
    "                        transform = None, \n",
    "                        max_seq_length = max_seq_len, \n",
    "                        target_size = target_size\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append(\"../vilt/transforms/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import pixelbert.pixelbert_transform\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvilt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpixelbert\u001b[39;00m \u001b[39mimport\u001b[39;00m pixelbert_transform\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# import pixelbert.pixelbert_transform\n",
    "from ..vilt.transforms.pixelbert import pixelbert_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_transform = \n",
    "\n",
    "def collate_fn(data_bunch):\n",
    "    '''\n",
    "    A function for the dataloader to return a batch dict of given keys\n",
    "\n",
    "    data_bunch: List of dictionary\n",
    "    '''\n",
    "    dict_data_bunch = {}\n",
    "\n",
    "    for i in data_bunch:\n",
    "        for (key, value) in i.items():\n",
    "            if key not in dict_data_bunch:\n",
    "                dict_data_bunch[key] = []\n",
    "            dict_data_bunch[key].append(value)\n",
    "\n",
    "    for key in list(dict_data_bunch.keys()):\n",
    "        dict_data_bunch[key] = torch.stack(dict_data_bunch[key], axis = 0)\n",
    "\n",
    "    if 'img' in dict_data_bunch:\n",
    "        ## Pre-processing for ViT\n",
    "        dict_data_bunch['img'] = vit_feat_extract(list(dict_data_bunch['img']),return_tensors = 'pt')['pixel_values']\n",
    "\n",
    "    return dict_data_bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset,  batch_size = 32):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size = self.batch_size, \n",
    "                    collate_fn = collate_fn, shuffle = True, num_workers = 2, pin_memory = True, persistent_workers = True)\n",
    "  \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size = self.batch_size, \n",
    "                    collate_fn = collate_fn, shuffle = False, num_workers = 2, pin_memory = True, persistent_workers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mobile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "502b9a658dcc54ec90dd947290c105ef43adeb5ca12481f28a2784481d6c77ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
