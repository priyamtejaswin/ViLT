{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/tra220029p/tejaswin/miniconda3/envs/mobile/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import convert_ans_to_token, convert_ques_to_token, rotate, convert_token_to_ques, convert_token_to_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_json_file, get_specific_file, resize_align_bbox, get_tokens_with_boxes, create_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_BOX = [0, 0, 0, 0]\n",
    "max_seq_len = 512\n",
    "batch_size = 2\n",
    "target_size = (500,384) ## Note that, ViT would make it 224x224 so :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"../../textvqa_eval/\"\n",
    "\n",
    "train_ocr_json_path = os.path.join(base_path, 'TextVQA_Rosetta_OCR_v0.2_train.json')\n",
    "train_json_path = os.path.join(base_path, 'TextVQA_0.5.1_train.json')\n",
    "\n",
    "val_ocr_json_path = os.path.join(base_path, 'TextVQA_Rosetta_OCR_v0.2_val.json')\n",
    "val_json_path = os.path.join(base_path, 'TextVQA_0.5.1_val.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ocr_json = json.load(open(train_ocr_json_path))['data']\n",
    "train_json = json.load(open(train_json_path))['data']\n",
    "\n",
    "val_ocr_json = json.load(open(val_ocr_json_path))['data']\n",
    "val_json = json.load(open(val_json_path))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_df = pd.DataFrame(train_json)\n",
    "train_ocr_json_df = pd.DataFrame(train_ocr_json)\n",
    "\n",
    "val_json_df = pd.DataFrame(val_json)\n",
    "val_ocr_json_df = pd.DataFrame(val_ocr_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json_df['answers'] = train_json_df['answers'].apply(lambda x: \" \".join(list(map(str, x))))\n",
    "val_json_df['answers']   = val_json_df['answers'].apply(lambda x: \" \".join(list(map(str, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34602/34602 [00:00<00:00, 90623.35it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 84089.91it/s]\n"
     ]
    }
   ],
   "source": [
    "## Dropping of the images which doesn't exist, might take some time\n",
    "\n",
    "base_img_path = os.path.join(base_path, 'train_images')\n",
    "\n",
    "train_json_df['path_exists'] = train_json_df['image_id'].progress_apply(lambda x: os.path.exists(os.path.join(base_img_path, x)+'.jpg'))\n",
    "train_json_df = train_json_df[train_json_df['path_exists']==True]\n",
    "\n",
    "val_json_df['path_exists'] = val_json_df['image_id'].progress_apply(lambda x: os.path.exists(os.path.join(base_img_path, x)+'.jpg'))\n",
    "val_json_df = val_json_df[val_json_df['path_exists']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping the unused columns\n",
    "\n",
    "train_json_df.drop(columns = ['flickr_original_url', 'flickr_300k_url','image_classes', 'question_tokens', 'path_exists'\n",
    "                              ], axis = 1, inplace = True)\n",
    "val_json_df.drop(columns = ['flickr_original_url', 'flickr_300k_url','image_classes', 'question_tokens', 'path_exists'\n",
    "                              ], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting the json\n",
    "\n",
    "del train_json\n",
    "del train_ocr_json\n",
    "del val_json\n",
    "del val_ocr_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping for the purpose of feature extraction\n",
    "grouped_df = train_json_df.groupby('image_id')\n",
    "\n",
    "## Getting all the unique keys of the group by object\n",
    "keys = list(grouped_df.groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataset class for TextVQA\n",
    "\n",
    "class TextVqaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_img_path, json_df, ocr_json_df, tokenizer, \n",
    "    transform = None, max_seq_length = 100, target_size = (500,384), fine_tune = True):\n",
    "        self.base_img_path = base_img_path\n",
    "        self.json_df = json_df\n",
    "        self.ocr_json_df = ocr_json_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.fine_tune = fine_tune\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        curr_img = self.json_df.iloc[idx]['image_id']\n",
    "        ocr_token = self.ocr_json_df[self.ocr_json_df['image_id']==curr_img]['ocr_info'].values.tolist()[0]\n",
    "\n",
    "        boxes = []\n",
    "        words = []\n",
    "\n",
    "        current_group = self.json_df.iloc[idx]\n",
    "        width, height = current_group['image_width'], current_group['image_height']\n",
    "\n",
    "        for entry in ocr_token:\n",
    "            xmin, ymin, w, h, angle = entry['bounding_box']['top_left_x'], entry['bounding_box']['top_left_y'],  entry['bounding_box']['width'],  entry['bounding_box']['height'], entry['bounding_box']['rotation']\n",
    "            xmin, ymin,w, h = resize_align_bbox([xmin, ymin, w, h], 1, 1, width, height)\n",
    "            \n",
    "            x_centre = xmin + (w/2)\n",
    "            y_centre = ymin + (h/2)\n",
    "\n",
    "            ## print(\"The angle is:\", angle)\n",
    "            xmin, ymin = rotate([x_centre, y_centre], [xmin, ymin], angle)\n",
    "\n",
    "            xmax = xmin + w\n",
    "            ymax = ymin + h\n",
    "\n",
    "            ## Bounding boxes are normalized\n",
    "            curr_bbox = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(curr_bbox)\n",
    "            words.append(entry['word'])\n",
    "\n",
    "        img_path = os.path.join(self.base_img_path, curr_img)+'.jpg'\n",
    "        assert os.path.exists(img_path)==True, f'Make sure that the image exists at {img_path}!!'\n",
    "\n",
    "        if self.fine_tune:\n",
    "            ## For fine-tune stage, they use [0, 0, 1000, 1000] for all the bounding box\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = img.resize(self.target_size)\n",
    "            boxes = torch.zeros(self.max_seq_length, 4)\n",
    "            boxes[:, 2] = 1000\n",
    "            boxes[:, 3] = 1000\n",
    "            \n",
    "            words = \" \".join(words)\n",
    "            tokenized_words = self.tokenizer.encode(words, max_length = self.max_seq_length, \n",
    "                truncation = True, padding = 'max_length', return_tensors = 'pt')[0]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Flow for `self.fine_tune != False` is not defined!\")\n",
    "\n",
    "        ## Converting the boxes as per the format required for model input\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.int32)\n",
    "        width = (boxes[:, 2] - boxes[:, 0]).view(-1, 1)\n",
    "        height = (boxes[:, 3] - boxes[:, 1]).view(-1, 1)\n",
    "        boxes = torch.cat([boxes, width, height], axis = -1)\n",
    "\n",
    "        ## Clamping the value,as some of the box values are out of bound\n",
    "        boxes[:, 0] = torch.clamp(boxes[:, 0], min = 0, max = 1000)\n",
    "        boxes[:, 2] = torch.clamp(boxes[:, 2], min = 0, max = 1000)\n",
    "        boxes[:, 4] = torch.clamp(boxes[:, 4], min = 0, max = 1000)\n",
    "        \n",
    "        boxes[:, 1] = torch.clamp(boxes[:, 1], min = 0, max = 1000)\n",
    "        boxes[:, 3] = torch.clamp(boxes[:, 3], min = 0, max = 1000)\n",
    "        boxes[:, 5] = torch.clamp(boxes[:, 5], min = 0, max = 1000)\n",
    "        \n",
    "        ## Tensor tokenized words\n",
    "        tokenized_words = torch.as_tensor(tokenized_words, dtype=torch.int32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "\n",
    "        ## Getting the Question\n",
    "        question = current_group['question']   \n",
    "        question = convert_ques_to_token(question = question, tokenizer = self.tokenizer)\n",
    "\n",
    "        ## Getting the Answer\n",
    "        answer = current_group['answers']\n",
    "        answer = convert_ques_to_token(question = answer, tokenizer = self.tokenizer).long()\n",
    "\n",
    "        return {'img':img, 'boxes': boxes, 'tokenized_words': tokenized_words, \n",
    "                'question': question, 'answer': answer, 'id': torch.as_tensor(idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \n",
    "\n",
    "train_ds = TextVqaDataset(base_img_path = base_img_path,\n",
    "                         json_df = train_json_df,\n",
    "                         ocr_json_df = train_ocr_json_df,\n",
    "                         tokenizer = tokenizer,\n",
    "                         transform = None, \n",
    "                         max_seq_length = max_seq_len, \n",
    "                         target_size = target_size\n",
    "                         )\n",
    "\n",
    "\n",
    "val_ds = TextVqaDataset(base_img_path = base_img_path,\n",
    "                        json_df = val_json_df,\n",
    "                        ocr_json_df = val_ocr_json_df,\n",
    "                        tokenizer = tokenizer,\n",
    "                        transform = None, \n",
    "                        max_seq_length = max_seq_len, \n",
    "                        target_size = target_size\n",
    "                        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mobile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "502b9a658dcc54ec90dd947290c105ef43adeb5ca12481f28a2784481d6c77ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
